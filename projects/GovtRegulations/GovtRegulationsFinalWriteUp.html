<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url('https://themes.googleusercontent.com/fonts/css?kit=pYEwJuzr3ZMDX2Y1syVbvx06if6osnyAslCuLPPf50A');.lst-kix_f9pu79ulaxvd-2>li:before{content:"\0025a0  "}.lst-kix_f9pu79ulaxvd-1>li:before{content:"\0025cb  "}.lst-kix_z8vngxg23rtb-1>li:before{content:"\0025cb  "}.lst-kix_z8vngxg23rtb-2>li:before{content:"\0025a0  "}.lst-kix_f9pu79ulaxvd-0>li:before{content:"\0025cf  "}.lst-kix_f9pu79ulaxvd-6>li:before{content:"\0025cf  "}.lst-kix_f9pu79ulaxvd-5>li:before{content:"\0025a0  "}.lst-kix_f9pu79ulaxvd-7>li:before{content:"\0025cb  "}.lst-kix_z8vngxg23rtb-0>li:before{content:"\0025cf  "}ul.lst-kix_5am1e5y0co6s-7{list-style-type:none}ul.lst-kix_5am1e5y0co6s-8{list-style-type:none}ul.lst-kix_5am1e5y0co6s-5{list-style-type:none}ul.lst-kix_5am1e5y0co6s-6{list-style-type:none}.lst-kix_f9pu79ulaxvd-3>li:before{content:"\0025cf  "}ul.lst-kix_5am1e5y0co6s-3{list-style-type:none}ul.lst-kix_5am1e5y0co6s-4{list-style-type:none}ul.lst-kix_5am1e5y0co6s-1{list-style-type:none}ul.lst-kix_5am1e5y0co6s-2{list-style-type:none}.lst-kix_f9pu79ulaxvd-4>li:before{content:"\0025cb  "}.lst-kix_9nmmhav109jp-7>li:before{content:"-  "}.lst-kix_9nmmhav109jp-8>li:before{content:"-  "}.lst-kix_9nmmhav109jp-5>li:before{content:"-  "}.lst-kix_9nmmhav109jp-6>li:before{content:"-  "}.lst-kix_5am1e5y0co6s-0>li:before{content:"\0025cf  "}.lst-kix_z8vngxg23rtb-8>li:before{content:"\0025a0  "}.lst-kix_5am1e5y0co6s-1>li:before{content:"\0025cb  "}.lst-kix_9nmmhav109jp-3>li:before{content:"-  "}.lst-kix_9nmmhav109jp-4>li:before{content:"-  "}.lst-kix_5am1e5y0co6s-3>li:before{content:"\0025cf  "}.lst-kix_5am1e5y0co6s-2>li:before{content:"\0025a0  "}.lst-kix_z8vngxg23rtb-3>li:before{content:"\0025cf  "}ul.lst-kix_z8vngxg23rtb-8{list-style-type:none}ul.lst-kix_z8vngxg23rtb-7{list-style-type:none}ul.lst-kix_z8vngxg23rtb-6{list-style-type:none}ul.lst-kix_z8vngxg23rtb-5{list-style-type:none}ul.lst-kix_z8vngxg23rtb-4{list-style-type:none}ul.lst-kix_z8vngxg23rtb-3{list-style-type:none}.lst-kix_z8vngxg23rtb-4>li:before{content:"\0025cb  "}ul.lst-kix_z8vngxg23rtb-2{list-style-type:none}.lst-kix_z8vngxg23rtb-7>li:before{content:"\0025cb  "}.lst-kix_z8vngxg23rtb-5>li:before{content:"\0025a0  "}.lst-kix_z8vngxg23rtb-6>li:before{content:"\0025cf  "}ul.lst-kix_9nmmhav109jp-0{list-style-type:none}ul.lst-kix_9nmmhav109jp-1{list-style-type:none}ul.lst-kix_z8vngxg23rtb-1{list-style-type:none}ul.lst-kix_9nmmhav109jp-2{list-style-type:none}ul.lst-kix_z8vngxg23rtb-0{list-style-type:none}ul.lst-kix_9nmmhav109jp-3{list-style-type:none}ul.lst-kix_9nmmhav109jp-4{list-style-type:none}ul.lst-kix_9nmmhav109jp-5{list-style-type:none}ul.lst-kix_9nmmhav109jp-6{list-style-type:none}.lst-kix_ncf1ii9hmyz-6>li:before{content:"\0025cf  "}ul.lst-kix_9nmmhav109jp-7{list-style-type:none}ul.lst-kix_9nmmhav109jp-8{list-style-type:none}.lst-kix_5am1e5y0co6s-5>li:before{content:"\0025a0  "}.lst-kix_9nmmhav109jp-0>li:before{content:"-  "}.lst-kix_9nmmhav109jp-1>li:before{content:"-  "}.lst-kix_9nmmhav109jp-2>li:before{content:"-  "}.lst-kix_ncf1ii9hmyz-7>li:before{content:"\0025cb  "}.lst-kix_5am1e5y0co6s-4>li:before{content:"\0025cb  "}.lst-kix_5am1e5y0co6s-8>li:before{content:"\0025a0  "}.lst-kix_ncf1ii9hmyz-8>li:before{content:"\0025a0  "}.lst-kix_5am1e5y0co6s-7>li:before{content:"\0025cb  "}.lst-kix_5am1e5y0co6s-6>li:before{content:"\0025cf  "}ul.lst-kix_ncf1ii9hmyz-2{list-style-type:none}ul.lst-kix_5am1e5y0co6s-0{list-style-type:none}ul.lst-kix_ncf1ii9hmyz-3{list-style-type:none}ul.lst-kix_ncf1ii9hmyz-4{list-style-type:none}ul.lst-kix_ncf1ii9hmyz-5{list-style-type:none}ul.lst-kix_ncf1ii9hmyz-6{list-style-type:none}ul.lst-kix_ncf1ii9hmyz-7{list-style-type:none}ul.lst-kix_ncf1ii9hmyz-8{list-style-type:none}.lst-kix_f9pu79ulaxvd-8>li:before{content:"\0025a0  "}ul.lst-kix_ncf1ii9hmyz-0{list-style-type:none}ul.lst-kix_ncf1ii9hmyz-1{list-style-type:none}.lst-kix_ncf1ii9hmyz-4>li:before{content:"\0025cb  "}.lst-kix_ncf1ii9hmyz-5>li:before{content:"\0025a0  "}ul.lst-kix_f9pu79ulaxvd-8{list-style-type:none}ul.lst-kix_f9pu79ulaxvd-7{list-style-type:none}ul.lst-kix_f9pu79ulaxvd-6{list-style-type:none}ul.lst-kix_f9pu79ulaxvd-5{list-style-type:none}.lst-kix_ncf1ii9hmyz-3>li:before{content:"\0025cf  "}ul.lst-kix_f9pu79ulaxvd-4{list-style-type:none}ul.lst-kix_f9pu79ulaxvd-3{list-style-type:none}ul.lst-kix_f9pu79ulaxvd-2{list-style-type:none}ul.lst-kix_f9pu79ulaxvd-1{list-style-type:none}.lst-kix_ncf1ii9hmyz-0>li:before{content:"\0025cf  "}.lst-kix_ncf1ii9hmyz-1>li:before{content:"\0025cb  "}ul.lst-kix_f9pu79ulaxvd-0{list-style-type:none}.lst-kix_ncf1ii9hmyz-2>li:before{content:"\0025a0  "}ol{margin:0;padding:0}table td,table th{padding:0}.c2{orphans:2;widows:2;text-align:justify}.c1{orphans:2;widows:2;height:11pt}.c5{font-size:8pt;color:#1155cc;text-decoration:underline}.c7{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c19{margin-left:108pt;text-align:left}.c18{margin-left:36pt;padding-left:0pt}.c24{font-size:24pt;color:#666666}.c6{color:inherit;text-decoration:inherit}.c14{color:#1155cc;text-decoration:underline}.c10{orphans:2;widows:2}.c25{padding:0;margin:0}.c22{width:33%;height:1px}.c23{font-size:30pt}.c20{text-align:left}.c0{font-family:"Roboto"}.c16{line-height:1.0}.c12{color:#ff0000}.c3{font-style:italic}.c8{background-color:#ffffff}.c11{height:11pt}.c9{font-size:14pt}.c17{font-size:8pt}.c26{text-align:center}.c4{font-size:18pt}.c13{font-size:12pt}.c21{font-family:"Consolas"}.c15{font-size:10pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c7"><div><p class="c1"><span></span></p></div><p class="c10"><span class="c0 c23">Analyzing Notice &amp; Comment</span></p><p class="c10"><span class="c0 c3 c24">Assessing large volumes of comments on proposed government regulations</span></p><p class="c1"><span class="c0 c4"></span></p><p class="c1"><span class="c0 c4"></span></p><p class="c1"><span class="c0 c4"></span></p><p class="c1"><span class="c0 c4"></span></p><p class="c1"><span class="c0 c4"></span></p><p class="c1"><span class="c0 c4"></span></p><p class="c1"><span class="c0 c4"></span></p><p class="c1"><span class="c0 c4"></span></p><p class="c1"><span class="c0 c4"></span></p><p class="c1"><span class="c0 c4"></span></p><p class="c1"><span class="c0 c4"></span></p><p class="c1"><span class="c0 c4"></span></p><p class="c1"><span class="c0 c4"></span></p><p class="c1"><span class="c0 c4"></span></p><p class="c1"><span class="c0 c4"></span></p><p class="c1"><span class="c0 c4"></span></p><p class="c1"><span class="c0 c4"></span></p><p class="c1"><span class="c0 c4"></span></p><p class="c1"><span class="c0 c13"></span></p><p class="c1"><span class="c0 c13"></span></p><p class="c1"><span class="c0 c13"></span></p><p class="c1 c20"><span class="c0 c13"></span></p><p class="c10 c19"><span class="c0 c13">Kinshuk, Jason Danker, Emily Witt, &amp; Proxima DasMohapatra</span><hr style="page-break-before:always;display:none;"></p><p class="c10"><span class="c0 c4">0. Important Links</span></p><p class="c10"><span class="c0 c9"><br></span><span class="c0 c3 c9">Final Tool</span><span class="c0 c9"><br></span><span class="c14 c0"><a class="c6" href="https://www.google.com/url?q=https://nlp-notice-and-comment.herokuapp.com/index&amp;sa=D&amp;ust=1484162637674000&amp;usg=AFQjCNE0XWu7PFxUMLwWbnS4592R1PH5xg">https://nlp-notice-and-comment.herokuapp.com/index</a></span><span class="c0">&nbsp;</span></p><p class="c10"><span class="c0 c15"><br></span><span class="c0 c3 c9">Data</span><span class="c0 c15">&nbsp;<br></span><span class="c14 c0"><a class="c6" href="https://www.google.com/url?q=https://drive.google.com/drive/folders/0B2S0QXTCjpeLaHNYbzUtR0VqVU0?usp%3Dsharing&amp;sa=D&amp;ust=1484162637676000&amp;usg=AFQjCNFA5skC7Nv76c9bd8C80F28cfwsiw">https://drive.google.com/drive/folders/0B2S0QXTCjpeLaHNYbzUtR0VqVU0?usp=sharing</a></span><span class="c0">&nbsp;</span></p><p class="c10"><span class="c0 c15"><br></span><span class="c0 c3 c9">NBViewer Files</span></p><ul class="c25 lst-kix_f9pu79ulaxvd-0 start"><li class="c18 c10"><span class="c0">Extract Data </span><span class="c14 c0"><a class="c6" href="https://www.google.com/url?q=http://nbviewer.jupyter.org/gist/emilywitt/ae6da19656ec2f40dd81344a10f7fdc6&amp;sa=D&amp;ust=1484162637678000&amp;usg=AFQjCNERuAFqrMBj16s9e0HrTEKxHwqLRA">http://nbviewer.jupyter.org/gist/emilywitt/ae6da19656ec2f40dd81344a10f7fdc6</a></span></li><li class="c10 c18"><span class="c0">Document Summary </span><span class="c14 c0"><a class="c6" href="https://www.google.com/url?q=http://nbviewer.jupyter.org/gist/emilywitt/a7fd7ece14d25e07cd4393de3f8062bc&amp;sa=D&amp;ust=1484162637679000&amp;usg=AFQjCNEv3MEx_eM1v6FmlNpFtdc_jo4BoQ">http://nbviewer.jupyter.org/gist/emilywitt/a7fd7ece14d25e07cd4393de3f8062bc</a></span></li><li class="c18 c10"><span class="c0">Comment Clustering </span><span class="c0 c14"><a class="c6" href="https://www.google.com/url?q=http://nbviewer.jupyter.org/gist/emilywitt/368643cb2c9be6ed67498142fd974fd0&amp;sa=D&amp;ust=1484162637681000&amp;usg=AFQjCNFPQ1Js-JxUFihCwuRkuBGlV2ADHg">http://nbviewer.jupyter.org/gist/emilywitt/368643cb2c9be6ed67498142fd974fd0</a></span><span class="c0">&nbsp; </span></li></ul><p class="c1"><span class="c0 c4"></span></p><p class="c10"><span class="c0 c4">1. Project Goals</span></p><p class="c2 c11"><span class="c0 c3 c9"></span></p><p class="c2"><span class="c0 c3 c9">1.1 Original Intent</span><span class="c0"><br>Our goal was to develop an algorithm that would aid human analysts at </span><span class="c14 c0"><a class="c6" href="https://www.google.com/url?q=http://www.regulations.gov&amp;sa=D&amp;ust=1484162637683000&amp;usg=AFQjCNH0KL3X7I-O2rHRzOtgGYlmEdSECg">regulations.gov</a></span><span class="c0">&nbsp;in the process of summarizing and clustering comments submitted by citizens during each regulation&rsquo;s mandatory notice-and-comment period. The volume of comments in some proposed regulations can be in the hundreds of thousands, posing a challenge for analysts to manually review each comment and organize the areas of concern. We aim to aid policymakers in using this information to make better decisions as well as allow other interested parties, including concerned citizens, to explore the documents and the comments being submitted.</span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c0 c3 c9">1.2 How far we got</span></p><p class="c2"><span class="c0">We have created a web-based tool where users can explore extracted summary and categorization information for 35 regulatory documents. We had hoped to use the regulations.gov API to make this a live tool that visitors could use to explore any regulatory document on regulations.gov. However, the API is capped by number of calls per hour. &nbsp;This limits our ability to process multiple request for pulling a document with high comment volume.</span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c0">For the 35 documents we currently have available, we are displaying the human-written summary of the document, our algorithmically extracted keywords and important sentences from the document, as well as the top differentiating topics and the central comment from each cluster. This combination of extracted information gives the overall gist of both the regulatory document and the comments posted by citizens and advocates.</span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c0 c3 c9">1.3 Future work </span></p><p class="c2"><span class="c0">In the future, we would hope to make this tool available via API for analysts to use in reviewing dockets. It would also serve concerned citizens, residents, and advocates to understand the regulations as well as the concerns that the public has about them. With a live API, we could support more interactions, like searching for &nbsp;regulatory documents by title, agency or interest area of individual. These could then be processed and the results made available in real time.</span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c0">Additionally, we would like to include all comments but sort them according to closeness to the center of the cluster. This would allow analysts to review comments by order of relevancy without hiding or pruning any information. We could also add more context to the analysis by doing sentiment analysis on the comments in each cluster and showing most negative and positive comment as well as the overall sentiment in the cluster. Apart from this, context can also be added by showing the relative size of clusters. We hope that these additions would help make this tool more robust and valuable for analysts as well as concerned citizens.<br></span></p><p class="c2"><span class="c0">With these additions, we would also revisit the layout of our user interface. While we feel the current interface is sufficient for the prototype, adding in this additional information, especially the inclusion of all comments, would likely result in the user being overwhelmed. In the redesign, we would take into account the research findings from related work such as that by Brooks et al. in &lsquo;Divide and Correct: Using Clusters to Grade Short Answers at Scale.&rsquo; Providing a UI similar to theirs would allow analysts to more quickly review the comments and make notes as they go along.</span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c0 c3 c9">1.4 </span><span class="c0 c3 c9">Results/Evaluation</span></p><p class="c2"><span class="c0">Users evaluated our document summary algorithm and our comment analysis algorithm. For the document summary algorithm, we asked users to assign a representativeness scores to the keywords and extracted sentences for a single document by comparing the algorithmically generated outputs to the human-written summary from the document itself. For the comment analysis algorithm, we asked users to assess the similarity between the central comment and randomly selected comments within a </span><span class="c0">cluster</span><span class="c0">, as well as the representativeness of the cluster keywords based on all the document comments they were shown. We also asked open-ended questions to elicit feedback about whether our tool provides additional helpful context and value beyond what the human summary provides.</span><sup class="c0"><a href="#ftnt1" id="ftnt_ref1">[1]</a></sup></p><p class="c2 c11"><span class="c0"></span></p><p class="c10 c26"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 580.50px; height: 467.49px;"><img alt="NLP_DB3.png" src="images/image00.png" style="width: 580.50px; height: 467.49px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c0">We had 16 responses, though some were partial responses due to fatigue from the amount of text the respondents had to read through. We hoped to have some agency analysts evaluate our tool and we reached out to several of the contacts listed under &ldquo;For Information Contact&rdquo; from the most recent of the documents we analyzed, as well as several contacts from dockets that are currently open but which we didn&rsquo;t analyze. Unfortunately, we did not receive responses from the analysts, but ideally we would have them evaluate our tool in addition to non-agency users.</span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c0">Our average document sentence scores ranged from 2-3.5, with the lowest scores coming from the &ldquo;Seniors&rdquo; document whose sentences were by far the longest of the three documents we asked users to evaluate. This indicates we may want to tweak our algorithm, so as to to return shorter sentences, but we think optimal length may differ depending on if the user was an agency analyst. We would like to test the tool with this user group before making a change.</span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c0">Our document keywords were scored more highly than our extracted sentences and our open-ended questions provide more insight into why our users assigned the scores they did. One evaluator wrote that &ldquo;The Canada piece is not represented in the summary and seems useful to know about&rdquo; highlighting a keyword we extracted that provided additional context beyond the human-written summary but was not included in our extracted sentences.</span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c0">However, our comment cluster keyphrases scored slightly lower than our document keywords, and two evaluators (one for &ldquo;Bovine Encephalopathy&rdquo; and one for &ldquo;Shale Gas Extraction&rdquo;) noted that these keyphrases included &ldquo;arcane&rdquo; or &ldquo;niche&rdquo; words. This made it difficult for a non-subject-matter-expert to tell whether they were representative of the document or not and also did not expand the individual&rsquo;s understanding of the comments. This seems an interesting insight into the comments: given that the comment keyphrases were taken from comments written by citizens, we would have expected a limited amount of technical jargon. While we could expect agency analysts analysts to have more context and knowledge of technical jargon, we would need additional testing with that user base to determine if they would experience similar issues.</span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c0">Regardless, an interesting expansion of this project would be to cluster comments according to whether they appear to be written by experts or non-experts, to get a sense for who is commenting and what groups may, or may not, be under-represented in the process. Given this was especially relevant for two of our documents that seem more technical than the other document, this may change based on the type of regulation, the agency, or the parties directly impacted by the regulation.</span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c0">Finally, the average score for comment cluster similarity was just above neutral for all documents, with one evaluator explaining that the comments &ldquo;failed to represent the complete &#39;central&#39; comment&rdquo;, indicating that perhaps more guidance was needed on what we meant by &ldquo;similar&rdquo;, especially as we were asking participants to switch to a new measure halfway through the survey.</span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c0">Additionally, some feedback we received on the evaluation itself was that people had the desire to rate keywords individually, and that it was too long. This second point we understand but believe that it is another reason why agency analysts would be the ideal user to evaluation the tool, as they routinely read through much more text than this in the process of reviewing comments on their dockets. At the same time, this highlights the difficulties of attempting to design a tool for two user groups: citizens want a quick high-level overview while analysts necessarily need to be in the weeds of the regulations and the respective comments.</span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c0 c3">Note:</span><span class="c0">&nbsp;We originally planned to evaluate the algorithms by asking users to measure how confident they were in making decisions about the regulation based on the extracted information and comment clustering. However, we received feedback from Professor Hearst that user confidence may not be the best measure as users simply don&rsquo;t know what they don&rsquo;t know, and have a difficult time predicting hypothetical future behavior.</span></p><p class="c2 c11"><span class="c0 c4"></span></p><p class="c2"><span class="c0 c4">2. Description of data</span></p><p class="c2 c11"><span class="c0 c9"></span></p><p class="c2"><span class="c0 c3 c9">2.1 Regulations.gov API</span></p><p class="c2"><span class="c0">All of our data was collected through the Regulations.gov API </span><span class="c0 c8">which provides an interface to access regulation text and related comments from within a docket. </span><span class="c0">While the API initially promised to be a convenient way to get the data we needed from the site, we ran into several problem with API call limit of 1,000 calls per hour, an inconsistent data model, and the format of the data itself.</span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c0">To get all the documents, we first collected the document IDs from the ten categories of regulations on the site. We then extracted the docket id from each document and queried the docket. This docket query gave us a link for the document which required another call. An additional call was required to get every batch of 1,000 comments that were written directly in the text area, and two calls were needed to get each attached comment. We calculated the range of calls (from best to worst case) for a document with n number of comments:</span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c15 c21">1 (open url) + 1 (doc text) + 1 (comment count) + n/1000 (divide by results per page) + [0 , n] (attachment) + [0 , n] (open attachment) = total number of calls allowed</span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c0">The sheer number of calls to retrieve one coherent docket with the comment and regulation text limited the scope of our project. While we had initially hoped for a user to be able to search for a piece of regulation which we would then process in real time, the restrictive API made this impossible. We tried to get around this problem by requesting higher rate on API from the Regulation.gov tech team and were given up to 2,500 calls per hour, but only for a limited period. Since the purpose of the project was to make it easier for analysts to process documents with large numbers of comments, we decided download documents with the maximum number of comments we could get in an hour or single run of our algorithm without fail.</span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c0">Using the equation above, we calculated that in the worst case scenario (all comments have an attachment), we would be able to download one document with 498 comments in an hour with rate of 1,000 which later increased to one document with 1,247 comments with a rate of 2,500. While we had the higher call rate, we downloaded a number of different documents from the various regulation categories. We then used these documents to build our NLP models with the operating assumption that we could later scale the tool and expand its scope by negotiating for open API use with the site. Given all this, for the purposes of the assignment, we worked on </span><span class="c0">35</span><span class="c0">&nbsp;regulations from various domains and their comments.</span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c0 c3 c9">2.2 Regulation Data</span></p><p class="c2"><span class="c0">The regulation text was available as a HTML page that could be also be accessed via API. Since the page was formatted as HTML, we had to process the document in order to remove the HTML tags before we could use it. Apart from the tags, there were special symbols that were used to make tables and write references and notes. We had to write a regular expression that parsed through the document and removed anything that was not text and needed for analysis. The format of the regulation text differed from one document to another, and very few documents had complete metadata in terms of the docket number or table of content. However, we noticed that almost all the documents had same top level section heads called &ldquo;Summary&rdquo;, &ldquo;Dates&rdquo;, &ldquo;Address&rdquo; and &ldquo;supplementary information.&rdquo; The &ldquo;summary&rdquo; was a 4 to 5 line summary of the regulation and supplementary information contained more details on the regulation. We extracted both of these parts to use for further analysis.</span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c0 c3 c9">2.3 Comment Data</span></p><p class="c2"><span class="c0">Comments on the regulations were submitted to Regulations.gov through text areas on the site as well as through attachments. </span><span class="c0">While processing the text fields was straightforward, processing the attachments proved to be challenging. </span><span class="c0">Due to time a resource limitations during this project, we chose to ignore all attachments in formats other than PDF. We would look for ways to process all attachments in future iterations of the tool. Of the PDFs, some of were scanned and handwritten letters and drawings while others were pictures. Despite this, we found a (somewhat) reliable way to process the PDFs: we converted PDFs into text using the Python module PyPDF which was built to convert PDF files to plain text. Having downloaded and saved the attached comments in PDF format, we then ran them through PyPDF to get the text into a useable form for our analysis. While the results were not perfect, and all attachments processed in this way had some artifacts of the processing in them, the bulk of the PDFs were meaningfully converted to text, and we felt confident in using them in developing our tool.</span></p><p class="c2 c11"><span class="c0 c4"></span></p><p class="c2"><span class="c0 c4">3. Algorithms</span></p><p class="c2 c11"><span class="c0 c9"></span></p><p class="c2"><span class="c0 c3 c9">3.1 Document keyword extraction</span></p><p class="c2"><span class="c0">We started our analysis of the regulation text by extracting keywords from the supplementary information section of the text. To get the keywords we thought of a number of techniques that we might employ and debated if nouns or verbs would be more important in a regulatory context. Eventually we decided that the content of the regulation would be better represented by nouns as they would be the objects of the regulations. While verbs could represent the regulatory actions, we decided that nouns were more likely to differentiate regulations from one another.</span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c0">We also tried different algorithms to generalize the nouns up the wordnet tree so that no two keywords would represent same thing and we would get a gist of the common themes in the document. However, the strategy of generalizing over wordnet by looking for common parent gave results that did not distinguish one regulation from another, and also the generalized words hid the important terms and information originally extracted from the text. Additionally, the nouns of the documents were so specific and unique that there was not a significant problem with overlapping concepts in keywords.</span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c0">Given this, we went with extracting the common noun phrases from the document text with no further processing. Since the unigrams and bigrams were somewhat redundant, we decided to show our user only top unigrams. We also considered including trigrams, and other longer n-grams, but the low frequency of occurrence limited their usefulness in surfacing general information about the regulation.</span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c0">Having decided on using noun phrase unigrams, </span><span class="c0">we calculated the length of the list of unigrams (non-unique), counted the number of occurrences for each unique unigram, and then selected the top NP unigrams whose counts accounted for 30% of the total count of non-unique unigrams. We decided to use 30% as the cutoff based on an &lsquo;expansion&rsquo; of the Pareto principle and felt that this should be indicative of the majority of the regulation.</span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c0 c3 c9">3.2 Document key sentences extraction</span></p><p class="c2"><span class="c0">For more information about the regulation text, we decided to extract the most important sentences from the supplementary information in the text. To extract the &nbsp;key sentences from the regulatory document, we first tokenized the whole supplementary information part of the text into sentences and word tokens within them. We then scored each sentence based on a few criteria.</span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c0">The first feature we selected was the occurrence of top keywords. These keywords were both unigrams and bigrams that were fed to sentence extraction algorithm from the keyword extraction algorithm.</span><span class="c0">&nbsp;The idea was if a sentence has more of these keywords, then that sentence is more important as it talks about the key issues of the document. To operationalize this, we gave each sentence a score equal to the number of keywords in it.</span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c0">The next feature was a &ldquo;selected words&rdquo; feature in which we gave sentences a higher score if they had certain words like &#39;complaint&#39;, &#39;concern&#39;, &#39;documented&#39;, &#39;evidence&#39;, &#39;warn&rsquo;, in them.</span><span class="c0">&nbsp;The reasoning behind this was that these words indicate sentences that specifically call out the problems that the regulation is intends to solve. We could not include regulation specific words in this feature as we had to generalize the algorithm across different regulations from variety of areas. For our prototype of the tool, we manually chose the &ldquo;selected words&rdquo; based on reading through a couple of documents. While this manual selection of words helped improve the relevance of the extracted sentences, in future iterations we would want to source the &ldquo;selected words&rdquo; from agency analysts whose tacit knowledge of the regulatory process would allow for a more comprehensive list grounded in expert knowledge.</span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c0">The penultimate feature was length of the sentence itself. While crude, our reasoning was that the longer a sentence is, the more information it would contain. With this strategy we ran into problem of over scoring sentences which were very long. This diluted the effect of keywords and selected word score which were also very important. Therefore, instead of using the sentence length as a score directly, we came up with the concept of a stepped length score. In this concept, each sentence earned a score corresponding to its length </span><span class="c0">divided by 10</span><span class="c0">&nbsp;and rounded to the nearest integer with a maximum score of 10. This method rewarded the greater amount of information theoretically contained in longer sentences, but not out of proportion to reward by keywords and selected words.</span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c0">The last feature we used was whether or not a &lsquo;sentence&rsquo; was a transformed table. In the document text, there were a lot of tables with cost estimates, time estimates, and other pieces of quantitative information. Due to our text processing, all of these tables were tokenized as sentences and, in some cases, got selected as top sentence. However, while these tables are important for the regulations themselves, they were not useful in understanding the semantic meaning of the regulation. As such, we wanted to remove any tables from final set of important sentences and derived a &ldquo;table score&rdquo; to penalize sentences containing a table. While there was no explicit indication of a table, we noticed tables tended to have a lot of periods (to connect cells) as well a lot of numbers. Given this, to identify tables we calculated the percentage of the sentence that was made up of dots and numbers. If this percentage was greater than 9%, we assumed the sentence contained a table. We came up with this percentage as the cut off by trying this method on many different tables in the data. Once we identified a sentence with a table, we gave it a heavy negative score to offset any score increase it received from keywords or its length.</span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c0">After scoring all the sentences on these three criteria, we summed up the scores and extracted the sentences with the highest scores. We played with displaying different numbers of sentences in the results but ultimately decided to go with top 4 as it seemed to provide the right balance between readability and volume of information. We then displayed these top sentences based on their order of index in text, rather than their score, so as to better approximate the flow of information in the original document.</span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c0">Beyond the features we ended up including, we also experimented with using a ratio of keywords to the sentence length. While this intuitively makes sense, it ended up penalizing longer sentences and rewarding really short sentences with, not only fewer words, but also fewer keywords (e.g. a sentence of 3 words in which 2 are keywords, will receive a higher score than a sentence of 10 words in which 5 are keywords). These shorter sentences weren&#39;t necessarily better and conveyed much less information than their longer counterparts. In light of this, we decided to not use this method.</span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c0">The inspiration for most of the features used in sentence extraction was a paper from Xerox</span><sup class="c0"><a href="#ftnt2" id="ftnt_ref2">[2]</a></sup><span class="c0">&nbsp;that used various features from sentences to assign a score to each sentence in order to succinctly summarize a given document. The paper also talked about using a trainable summarizer in order to generate a summary. While we considered this, we ultimately decided against doing so as it required creating a gold summary for the documents and it would have limited our ability to scale the algorithm for a large collection of regulation documents. We also decided against using the existing summary on Regulations.gov as the gold standard since doing so would allow any biases that were part of the original summary would be replicated in our algorithm for surfacing key sentences.</span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c0 c3 c9">3.3 Comment </span><span class="c0 c3 c9">clustering</span></p><p class="c2"><span class="c0">In order to better surface topics being discussed in the comments on the regulatory documents, we created clusters using k-means after vectorizing the comments using TF-IDF. While this successfully created clusters of comments, the clusters themselves were not </span><span class="c0">very semantically illuminating, and we began looking for ways to better understand the semantic basis for them. Initially, we looked at various ngram frequency distributions for each cluster and noted numerous clusters had significant overlap between them; the distributions had simply re-surfaced the overall subject matter of the regulatory document. While this was somewhat useful in showing the comments were actually related to the regulation, it did not help surface the different ways in which commenters thought about and commented on the regulation.</span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c0">To address this, we decided to extract the center-most comment from each cluster to serve as the &lsquo;exemplar&rsquo; for that cluster. As many of the comments are extremely long, we experimented with summarizing these long comments. However, we ultimately decided against this practice </span><span class="c0">because</span><span class="c0">&nbsp;of the key principles outlined on Regulations.gov that provide the conceptual grounding for the notice-and-comment process. Focusing on the democratic principles of legitimacy, responsiveness, acceptance, and public interest, it became apparent that summarizing comments, no matter how effective, could undermine these principles and therefore the entire notice-and-comment process. Additionally, as &ldquo;agenc[ies] must base [their] reasoning and conclusions on the rulemaking record,&rdquo; agency analysts need to understand the comments within this broader record, and summarizing long comments is unlikely to help them do so.</span><sup class="c0"><a href="#ftnt3" id="ftnt_ref3">[3]</a></sup></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c0">Having extracted the center-most comment, we began experimenting with ways other than a frequency distribution to identify the differentiating topics in each cluster. As the majority of comments were roughly talking about the same thing, namely the regulation, we needed a way to highlight the most salient differences between the clusters. Given this requirement, using TF-IDF was a natural option. To operationalize this on the clusters, we merged all of the comments for each cluster into one long &ldquo;document&rdquo; and then ran a TF-IDF vectorizer across the resulting cluster corpus. From the resulting matrix, we selected the n-grams with the highest TF-IDF scores within each cluster as the key differentiating phrases.</span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c0">Another challenge we faced was designing a method for determining the number of comment clusters each regulatory document should have, and we considered a number of options before making our final decision. One option was to use the number of keyword hypernyms from the regulation that show up more than a certain number of times in the comments. We considered using hypernyms as this would help address issues of synonymy in the texts. However, we were concerned that using keywords from the regulation text could exclude topics that commenters were discussing, that they thought were important or relevant to the the regulation, but were not being present in the regulation text itself. One example of this is that in the comments on a regulation about smoking, &ldquo;children&rdquo; and &ldquo;secondhand&rdquo; occurred frequently, but they were not present in the document keywords. While the exact topics would be irrelevant in determining the number of clusters, this process could understate the number of comment clusters each document should have and end up condensing separate concerns into a single cluster.</span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c0">Ultimately, we still decided to base the number of cluster on the keywords from the main regulation text itself, but not by tying these keywords back to the comments. Our logic behind this was that in absence of any other evidence, we could assume that the comments are talking about the main topics in the document. As such, our process for determining this number of clusters closely resembles our algorithm for document keyword extraction.</span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c0">To get the number of clusters, we first extracted the top 30% unigram noun phrases from the text.</span><sup class="c0"><a href="#ftnt4" id="ftnt_ref4">[4]</a></sup><span class="c0">&nbsp;To make sure the noun phrases represented different topics, we collapsed the words to their common parent in wordnet. All the wordnet trees were based on the most appropriate synset of the word as determined by context disambiguation using the Lesk algorithm. We then used the count of common parents extracted from wordnet to determine the number of comment cluster in each document.</span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c0">Beyond k-means, we initially also considered </span><span class="c0">using k-mediods or LDA for our clustering but decided against them because</span><span class="c0">&nbsp;they offered limited additional value over k-means while introducing greater complexity into the algorithm. For k-mediods, which we did not end up implementing and testing, one benefit would have been that the algorithm converges on a specific data point as the center for a cluster. While this provides a theoretical benefit, in our implementation of k-means, the distance from the central most comment to the cluster center was minimal so would result in little applied value for us. For LDA, which we did implement and test, we were able to surface topic clusters in the comments, but we unable to easily relate this back to the original comments. Without being able to tie the topics back to the comments, our understanding of the semantic meaning of the clusters was limited to just the topics returned. Additionally, we considered using latent semantic analysis to reduce noise in the TF-IDF matrix prior to running k-means, and implemented it during testing, but decided against using it in our final algorithm due to the lack of semantic meaning in the resulting dimensions.</span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c0 c3 c9">3.4 Sentiment analysis of comment clusters</span></p><p class="c2"><span class="c0">We experimented with conducting </span><span class="c0">sentiment analysis on comments in order to help analysts perceive the positive or negative sentiment with which commenters were discussing each regulatory document. </span><span class="c0">To do this, we used the Stanford-NLP, which uses a recursive neural network to annotate sentences with sentiments. Using Stanford-NLP&rsquo;s sentiment annotator, we wrote code to aggregate sentence sentiment within comments, and then aggregate comment sentiments within the comment clusters that we created in our clustering algorithm.</span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c0">However, as we reviewed the sentiment, it became apparent that in some cases the comment might be negative towards the regulation, and sometimes the comment was not negative towards the regulation but the overall tone was negative as a result of negativity towards the problem the regulation was trying to solve. One example of this was in the &ldquo;FR 5597-P-02 Instituting Smoke- Free Public Housing&rdquo; comments, where many of the comments were negative in sentiment because they told stories expressing frustration with people smoking in or near public housing. Therefore, the sentiment towards the proposed regulation was positive even the the sentiment of the comment in isolation was negative. However, for some types of regulations, like those that proposed increases in fees, such as &ldquo;Photo Requirements for Pilot Certificates&rdquo; which proposed a $22 fee for processing photos, the sentiment across the cluster was negative and, looking into the comments themselves, this appeared to more closely reveal sentiment towards the regulation itself, since most people had negative views about having to pay a fee.</span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c0">Dealing with the variety in type of documents and content was a challenge in all of our algorithms, but made sentiment analysis particularly difficult. In the example of the smoke-free public housing regulation, including sentiment analysis on comments in our tool could actually confuse or mislead the analyst reviewing the docket. To address this we experimented with using IBM Watson Alchemy API, which allows for passing keywords in and analyzing the sentiment around those specific keywords. We used our keywords from the regulatory documents and from our clustering algorithm to assess sentiment about those keywords in the comments. While overall sentiment across all comments in a cluster may be mixed (rather than positive or negative), targeted keyword sentiment may be different even for keywords that appear in multiple clusters, which we thought could give more semantic meaning to the clusters themselves. However, as a group we decided against using this in our final algorithmic tool due to the opacity of the Alchemy API. In short, we have very limited insight into how the tool works, and therefore would not be comfortable using it in a tool for government analysts.</span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c0">Additionally, we initially planned to display a quantification of sentiment across a comment cluster &mdash; for example, if a cluster had 15 comments in it, we planned to show that 10 of those comments were negative, 1 was positive, and 4 were neutral. While this could be interesting for citizens using our tool, the rulemaking process for federal regulations expressly forbids analysts from using this sort of quantification to make decisions: the &ldquo;process &nbsp;is &nbsp;not &nbsp;like &nbsp;a &nbsp;ballot &nbsp;initiative &nbsp;or &nbsp;an &nbsp;up&#8208;or&#8208;down &nbsp;vote &nbsp;in &nbsp;a &nbsp;legislature. &nbsp; An &nbsp;agency is not permitted to base its final rule on the number of comments in support of the rule over those in opposition to it.&rdquo;</span><sup class="c0"><a href="#ftnt5" id="ftnt_ref5">[5]</a></sup><span class="c0">&nbsp;While agencies are able to take acceptance and resistance into account when analyzing comments, they are explicitly disallowed from taking any quantification of support or resistance into account in their decision making. As &nbsp;such, &nbsp;the &nbsp;algorithm &nbsp;should &nbsp;attempt &nbsp;to &nbsp;surface &nbsp;support &nbsp;and opposition in the comments, but should not quantify their relative levels.</span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c0 c3">Note:</span><span class="c0">&nbsp;Algorithm not in use on the </span><span class="c0">website</span><span class="c0">.</span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c0 c3 c9">3.5 Summarizing comment clusters</span></p><p class="c2"><span class="c0">As displaying some of the comments on the website was difficult due to their length, we considered generating a summary of each comment cluster to aid our understanding of the clusters. Initially, we tried using our key sentence extraction algorithm on the clusters but, as the algorithm was meant for well written documents and expected a clear structure in the document, it was a poor match for the comments which lacked any standardized structure.</span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c0">We also tried using text-tiling to surface the topics in the comment clusters, but ended up focusing our consideration on a method called non-linear text tiling presented by Carlos N. Silla Jr. et al. Their algorithm builds on the idea of text-tiling, but has no expectations of an inherent structure in the document.</span><sup class="c0"><a href="#ftnt6" id="ftnt_ref6">[6]</a></sup><span class="c0">&nbsp;As Silla et al. summarize their algorithm, &ldquo;the distance between two sentences is calculated based on the distance between all nouns that appear in the sentences&ldquo; (Silla et al. p1). Having derived these distances for all sentences in the aggregated comments, the sentences could be clustered based on the distances between them, and sentences could be extracted from the resulting clusters to summarize the aggregated comments.</span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c0">However, we decided not to include this approach in the final algorithm as, when asking users to evaluate the quality of comment summarization against comments in the clusters, we would essentially be asking them to rate the effectiveness of two algorithms against each other, rather than the actual comments. Given this, we decided to surface important topics using TF-IDF as previously mentioned.</span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c0 c3">Note:</span><span class="c0">&nbsp;Algorithm not in use on the </span><span class="c0">website</span><span class="c0">.</span></p><p class="c2 c11"><span class="c0 c4"></span></p><p class="c2"><span class="c0 c4">4. Contribution of team members</span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c0">All four of us worked together to decide on the key elements of our tool (document keywords and sentences, comment clustering and sentiment analysis) and the NLP methods we would explore to best implement those elements. We did several iterations of discussing features collaboratively, then tinkering individually, before coming back together as a group to iterate on our approach and problem-solve around issues that came up. For example, we collaborated intensively on how best to approach determining the number of clusters dynamically for each document. </span><span class="c0">We also worked together to determine how best to evaluate our algorithms</span><span class="c0">&nbsp;as well as on the writeup for the project.</span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c0">Kinshuk took the lead in working with the regulations.gov API to gather our data as well as clean and process it, then we each used her code to call documents from the API. She also took the lead on extracting keywords from the documents which also fed into determining number of clusters for each document. She also took the lead in building the website to host our tool.</span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c0">Proxima took the lead on extracting key sentences from the documents and trying to come up with different approaches to summarize comment clusters.</span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c0">Jason took the lead in clustering comments, identifying the central comments in each cluster, and extracting key differentiating phrases for each cluster as well as getting the website hosted on Heroku.</span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c0">Emily worked on sentiment analysis with debugging support from Kinshuk, and took the lead in evaluating the algorithms by creating the evaluation survey and subsequently gathering and interpreting the results.</span></p><hr class="c22"><div><p class="c10 c16"><a href="#ftnt_ref1" id="ftnt1">[1]</a><span class="c0 c17">&nbsp;The representativeness scale was from 1-Not representative to 5-Highly representative and the similarity scale was from 1-Not similar to 5-Highly similar.</span></p></div><div><p class="c10 c16"><a href="#ftnt_ref2" id="ftnt2">[2]</a><span class="c0 c17">&nbsp;A Trainable Document Summarizer, Kupiec, Pedersen &amp; Chen, Xerox Palo Alto Research Center, 1995</span></p></div><div><p class="c10 c16"><a href="#ftnt_ref3" id="ftnt3">[3]</a><span class="c0 c17">&nbsp;https://www.federalregister.gov/uploads/2011/01/the_rulemaking_process.pdf</span></p></div><div><p class="c10 c16"><a href="#ftnt_ref4" id="ftnt4">[4]</a><span class="c0 c17">&nbsp;As previously, we chose 30% as an &lsquo;expanded&rsquo; Pareto principle.</span></p></div><div><p class="c10 c16"><a href="#ftnt_ref5" id="ftnt5">[5]</a><span class="c0 c17">&nbsp;https://www.federalregister.gov/uploads/2011/01/the_rulemaking_process.pdf</span></p></div><div><p class="c10 c16"><a href="#ftnt_ref6" id="ftnt6">[6]</a><span class="c0 c17">&nbsp;Silla et al. - </span><span class="c0 c5"><a class="c6" href="https://www.google.com/url?q=https://www.cs.kent.ac.uk/people/staff/aaf/pub_papers.dir/Wksp-Tec-Info-Ling-Silla-2003.pdf&amp;sa=D&amp;ust=1484162637740000&amp;usg=AFQjCNE37yhpxoi0y1GlNiO-T0JJGr7Zxw">A Non-Linear Topic Detection Method for Text Summarization Using Wordnet</a></span></p></div></body></html>